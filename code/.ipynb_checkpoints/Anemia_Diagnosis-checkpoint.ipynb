{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uUgutK66zlw"
   },
   "source": [
    "# 1. Packages & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Pre-Processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess, sys\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import pingouin as pg\n",
    "#!pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Data Visualization\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "import gradio as gr\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier # LightGBM is 6 times faster than XGBoost.\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ML Model Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc, matthews_corrcoef, cohen_kappa_score, log_loss\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Other imports\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('once')\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category = FutureWarning)\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "try:\n",
    "    import ydata_profiling\n",
    "except ImportError:\n",
    "    install('ydata_profiling')\n",
    "\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Project 1: Anemia Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9H8zT8uQKhG"
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "Anemia dataset containing attributes Gender, Hemoglobin, MCHC, MCV, MCH and Results. This dataset is used to predict if a patient is likely to suffer from anemia. Machine learning binary classifier algorithm to be used.\n",
    "\n",
    "Gender:\n",
    "- 0 - male\n",
    "- 1 - female\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Hemoglobin (g/dl)**: Hemoglobin is a protein in your red blood cells that carries oxygen to your body's organs and tissues and transports carbon dioxide from your organs and tissues back to your lungs\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**MCH (pg)**: MCH is short for \"mean corpuscular hemoglobin.\" It's the average amount in each of your red blood cells of a protein called hemoglobin, which carries oxygen around your body.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**MCHC (g/dl)**: MCHC stands for mean corpuscular hemoglobin concentration. It's a measure of the average concentration of hemoglobin inside a single red blood cell.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**MCV (f/l)**: MCV stands for mean corpuscular volume. An MCV blood test measures the average size of your red blood cells.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Results:\n",
    "- 0- not anemic\n",
    "- 1-anemic\n",
    "\n",
    "Kaggle link: https://www.kaggle.com/datasets/biswaranjanrao/anemia-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334,
     "referenced_widgets": [
      "0385260673ab4cccacf476811aa025db",
      "533f005be92647c4bfec0ae074b3893c",
      "dbe3f514b3a248f4a88c6a52e074c591",
      "39c59d38c96f411abd45b8dafa91e649",
      "9464d9c7cd8645c3bac9b69c0a8cabd5",
      "698b69e7ade946fe9256180cd5ec8748",
      "24e04369764c4b758894bab119a9c45f",
      "789cddba888e42e798c1658d8e4a0f59",
      "8adb0cf3050e4d89abd250df50452063",
      "13e663c6090f433691569b6221036c6d",
      "257f9c25023941bbb1c0b40efcbf3535",
      "dd719ba24172406e9588098582d407da",
      "0f3cb6593c9d427c9d80a58be10a0793",
      "65d8842f46c34e9fbe4f01ae04612e6c",
      "57f3262402dd49aeae748a340d5a45f2",
      "0d10fb586e3c4e73bcf461ac0aa84241",
      "dead88ca51364640afa2c6a8e122328c",
      "c4cc5e8b68f648a8b926506c74113ad9",
      "9cdd51fc6c094d9f81ae6d1835a2381f",
      "c439d93335b54442acc1ac1fa6b5a001",
      "b3abe4893c534352a6eb9186d43dcb55",
      "61290350ff5a45eabb18a6f3afaf4376",
      "13196fc982c44bd3813fcd66b59375da",
      "74032efc0ce545e38394a5fc79b846d4",
      "e71386c749494e85af1c9746b8fe2668",
      "ef7f34a7ce504612a4a7b7e76c034112",
      "41e0bbd249894d3ab858a27a74303247",
      "00864930f3454a13b78caa3677ad3a27",
      "bc8d2567584f4abd85e5004d0be6aac6",
      "2835cc263d6140918922c2d11d282bff",
      "7ecde3751b254e518174395f3426bedf",
      "52a263cd4eac4fafa82178e9ab98d848",
      "3caa876286da4695b9b8e49b1f1216fb",
      "4113b2e3720e44f3b7127fefd923302e",
      "2e7804c6251b4795b7fa64998c28e2bf",
      "d9e8d4fd7060436da8cf32a2f71cf209",
      "e0614a35d6474a8aaa91e2d2210dc648",
      "1dd6cb181a724b7386811fdd94bde728",
      "e88b70075e7847d79780839ed55bde84",
      "b605a76d4bbc4552811d5f206407bd5e",
      "297982b62f6e46ddbd2f9ac08f499eac",
      "1143bb161c5c4dec870974d1703ae369",
      "3c74df8e17bf4b8880d545b5324d93db",
      "2b13c1cca34f40e5823c831a72f2c90d"
     ]
    },
    "id": "a4IuX9J9OJu3",
    "outputId": "cdd2bf1d-69c0-45ed-c150-9164b72f7244"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/anemia.csv')\n",
    "\n",
    "# Generating a report of the data\n",
    "profile = ProfileReport(data, title = \"Anemia Dataset\")\n",
    "\n",
    "# Saving the report to .html for inspection\n",
    "profile.to_file(\"anemia_dataset_characteristics.html\")\n",
    "\n",
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upon reviewing the report, the following information is derived:**\n",
    "\n",
    "* The dataset comprises 6 columns/features (2 categorical and 4 numeric) with a total of 1421 observations with 0% missing data.\n",
    "\n",
    "* The 'Gender' column is labeleld as categorical in the report, but upon observing it we say it consists of only two unique values (0 - male & 1 - female). Same is applicable for the 'Result' column.\n",
    "\n",
    "* There are 472 duplicate rows, accounting for 33.2% of the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.iloc[[0, 1]])\n",
    "print((data.iloc[0] - data.iloc[1]).abs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzK_WZX9tHUB"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "_o_MmtDQREtD",
    "outputId": "efe42388-916a-42e0-cb19-52158128dba7"
   },
   "outputs": [],
   "source": [
    "# checking the types of the columns in the dataset\n",
    "print(data.dtypes)\n",
    "\n",
    "# checking for missing data in the columns of the dataset\n",
    "print('\\n', data.isna().sum(), '\\n')\n",
    "\n",
    "# dataset characteristics\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countplot(col, title, xlabel, ylabel, hue = None):\n",
    "\n",
    "    plt.figure()  # Starts a new figure\n",
    "    ax = sns.countplot(x = col, data = data, hue = hue)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.xticks(ticks = [0, 1], labels = ['Male', 'Female'])\n",
    "    plt.ylabel(ylabel)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "    \n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha = 'center', va = 'baseline', fontsize = 10, color = 'black', xytext = (0, 2),\n",
    "                    textcoords = 'offset points')\n",
    "\n",
    "# gender distribution\n",
    "countplot('Gender', 'Gender Distribution in the Dataset', 'Gender', '# of Patients')\n",
    "\n",
    "# gender distribution of anemia cases\n",
    "countplot('Gender', 'Gender Distribution in the Dataset', 'Gender', '# of Patients', hue = 'Result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is slight imbalance in the dataset (740 female cases vs 681 male cases).\n",
    "\n",
    "* **Nearly twice as many females have anemia compared to males.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anemia_cases = data[data['Result'] == 1]\n",
    "\n",
    "female_anemia_cases = anemia_cases[anemia_cases['Gender'] == 1]\n",
    "male_anemia_cases = anemia_cases[anemia_cases['Gender'] == 0]\n",
    "\n",
    "print(\"Female anemia cases as proportion of all anemia cases: \", np.round(len(female_anemia_cases)/len(anemia_cases), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1DdApeHhrarO",
    "outputId": "00ae3b85-b96a-44e9-b6b8-58fd3b355389"
   },
   "outputs": [],
   "source": [
    "# generating KDE plots for the other columns in the dataset\n",
    "columns_to_plot = [col for col in data.columns if col not in ['Gender', 'Result']]\n",
    "\n",
    "def plot_gender_specific(columns, data, gender_col = 'Gender'):\n",
    "    \n",
    "    for col in columns:\n",
    "        plt.figure()\n",
    "\n",
    "        # Check if the column is categorical or numerical\n",
    "        if data[col].dtype == 'object' or data[col].nunique() < 10:\n",
    "            sns.countplot(x = col, data = data, hue = gender_col)\n",
    "            plt.title(f'Distribution of {col} by Gender')\n",
    "            plt.ylabel('# of Cases')\n",
    "        else:\n",
    "            sns.histplot(data = data, x = col, hue = gender_col, kde = True, element = 'step', stat = 'density', common_norm = False)\n",
    "            plt.title(f'Distribution of {col} by Gender')\n",
    "            plt.ylabel('Density')\n",
    "        \n",
    "        plt.xlabel(col)\n",
    "        plt.legend(title = 'Gender', labels = ['Male', 'Female'])\n",
    "        plt.show()\n",
    "\n",
    "plot_gender_specific(columns_to_plot, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (heatmap)\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hemoglobin vs. Result (-0.8): There is a strong negative correlation, indicating that as the hemoglobin increases, the 'Result' tends to decrease, meaning that: \"the higher the hemoglobin, the lower the chance of having anemia\".**\n",
    "\n",
    "Gender vs. Result (0.25): There is a weak positive correlation, meaning there is a slight tendency for the 'Result' to increase as 'Gender' increases, though this relationship is not strong. This means that gender has some statistical importance when determining whether a patient has anemia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GEO4Zzotn2U"
   },
   "source": [
    "### Data Limitations\n",
    "\n",
    "* **DUPLICATE ROWS**\n",
    "* no patient's age provided\n",
    "* no information about other potential illnesses of a patient, nor medical history\n",
    "* no data on:\n",
    "  - MPV\n",
    "  - RDWc\n",
    "  - GRA%\n",
    "  - LYM%\n",
    "  - GRA\n",
    "  - MID\n",
    "  - LYM\n",
    "  - thrombocytes\n",
    "  - leukocytes\n",
    "  - erythrocytes\n",
    "  - hematocrits\n",
    "  - platelets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2BpZTuctZCA"
   },
   "source": [
    "## Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pT6S0O4PxXbT",
    "outputId": "9ad1b0f5-85ba-41b0-b17b-08f57e30e9b6"
   },
   "outputs": [],
   "source": [
    "# train features (X) and target (y)\n",
    "X = data.drop('Result', axis = 1)\n",
    "y = data['Result']\n",
    "\n",
    "# Splitting the data into training and testing sets (80% train & 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fitting a Logistic Regression Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "importance = model.coef_\n",
    "\n",
    "for i,v in enumerate(importance):\n",
    " print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model's performance\n",
    "print(f'Accuracy: {np.round(accuracy_score(y_test, y_pred) * 100, 2)}%')\n",
    "print(f'Precision: {np.round(precision_score(y_test, y_pred) * 100, 2)}%')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1 Score: {np.round(f1_score(y_test, y_pred), 2)}\\n')\n",
    "\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, y_pred), '\\n')\n",
    "\n",
    "# ROC Curve and AUC value\n",
    "print(\"ROC Curve: \")\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label = f'ROC curve (area = {auc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.show()\n",
    "\n",
    "print('\\nMatthews Correlation Coefficient:', np.round(matthews_corrcoef(y_test, y_pred),2))\n",
    "\n",
    "print('Cohen\\'s Kappa Score:', np.round(cohen_kappa_score(y_test, y_pred), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_data = data[data['Gender'] == 1]\n",
    "male_data = data[data['Gender'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNnqEqVl0J-k"
   },
   "source": [
    "## Testing on patient data\n",
    "\n",
    "Gender: Female (1)\n",
    "\n",
    "Hemoglobin: 12.4 (g/dl)\n",
    "\n",
    "MCH: 31.8 (pg)\n",
    "\n",
    "MCV: 77 (f/l)\n",
    "\n",
    "MCHC: 41.1 (g/dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yh9H-bk60Xjd",
    "outputId": "4a8ad2f4-0071-4d53-f4c8-92bdbc592a5f"
   },
   "outputs": [],
   "source": [
    "gender = int(input(\"Enter the Gender (0 for Male, 1 for Female): \"))\n",
    "hemoglobin = float(input(\"Enter the Hemoglobin value: \"))\n",
    "mch = float(input(\"Enter the MCH value: \"))\n",
    "mcv = float(input(\"Enter the MCV value: \"))\n",
    "mchc = float(input(\"Enter the MCHC value: \"))\n",
    "\n",
    "new_data = pd.DataFrame({'Gender': [gender], 'Hemoglobin': [hemoglobin], 'MCH': [mch], 'MCHC': [mchc], 'MCV': [mcv]})\n",
    "new_data_scaled = scaler.transform(new_data)\n",
    "\n",
    "prediction = model.predict(new_data_scaled)\n",
    "\n",
    "if prediction == 0:\n",
    "  print(\"The patient is predicted to NOT have anemia.\")\n",
    "else:\n",
    "  print(\"Model Outcome: The patient is predicted to have anemia.\")\n",
    "\n",
    "females_with_anemia = female_data[female_data['Result'] == 1]\n",
    "# Create a new DataFrame combining the user's data and the original data\n",
    "combined_data_anemia = pd.concat([females_with_anemia, new_data], ignore_index = True)\n",
    "combined_data_no_anemia = pd.concat([female_data, new_data], ignore_index = True)\n",
    "\n",
    "# Loop through features and create plots\n",
    "for feature in ['Hemoglobin', 'MCH', 'MCV', 'MCHC']:\n",
    "    plt.figure(figsize = (8, 5))\n",
    "\n",
    "    # Plot the histogram with KDE\n",
    "    sns.histplot(combined_data_no_anemia[feature], kde = True)\n",
    "\n",
    "    # Highlight user's data as a dotted vertical line\n",
    "    plt.axvline(new_data[feature].values[0], color = 'red', linestyle = '--', label = 'Patient Data')\n",
    "\n",
    "    plt.xlim(combined_data_no_anemia[feature].min(), combined_data_no_anemia[feature].max())\n",
    "    plt.title(f'Distribution of {feature} for females')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ktlHXS2a-QE"
   },
   "source": [
    "# Project 2: Anemia Type Classification\n",
    "\n",
    "**Data Overview**\n",
    "\n",
    "HGB: The amount of hemoglobin in the blood, crucial for oxygen transport.\n",
    "\n",
    "PlT: The number of platelets in the blood, involved in blood clotting.\n",
    "\n",
    "WBC: The count of white blood cells, vital for immune response.\n",
    "\n",
    "RBC: The count of red blood cells, responsible for oxygen transport.\n",
    "\n",
    "MCV (Mean Corpuscular Volume): Average volume of a single red blood cell.\n",
    "\n",
    "MCH (Mean Corpuscular Hemoglobin): Average amount of hemoglobin per red blood cell.\n",
    "\n",
    "MCHC (Mean Corpuscular Hemoglobin Concentration): Average concentration of hemoglobin in red blood cells.\n",
    "\n",
    "PDW: a measurement of the variability in platelet size distribution in the blood\n",
    "\n",
    "PCT: A procalcitonin test can help your health care provider diagnose if you have sepsis from a bacterial infection or if you have a high risk of developing sepsis\n",
    "\n",
    "Diagnosis: Anemia type based on the CBC parameters\n",
    "\n",
    "## 2.1 Exploratory Data Analysis\n",
    "\n",
    "### 2.1.1. Understanding the distributions for each prdictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anemia_data = pd.read_csv('../data/diagnosed_cbc_data_v4-original_data.csv')\n",
    "\n",
    "print(f\"Shape of the original dataframe: {anemia_data.shape}\\n\")\n",
    "\n",
    "# Box plot function for each predictor\n",
    "def plot_box(predictor):\n",
    "    fig = px.box(data_frame = anemia_data, \n",
    "                 y = predictor, \n",
    "                 color = 'Diagnosis', \n",
    "                 width = 800, height = 500, \n",
    "                 template = 'plotly_dark', \n",
    "                 title = f'Distribution of {predictor} by Diagnosis')\n",
    "    fig.update_layout(xaxis = dict(title = 'Diagnosis'), yaxis = dict(title = predictor), legend_title_text = 'Diagnosis')\n",
    "    fig.show()\n",
    "\n",
    "Xtrain = anemia_data.drop(columns = ['Diagnosis'])  # all columns except 'Diagnosis' are predictors\n",
    "\n",
    "# Looping through all predictors and creating box plots\n",
    "for var in Xtrain.columns:\n",
    "    plot_box(var)\n",
    "\n",
    "#anemia_data.drop(columns = ['LYMp', 'NEUTp'], inplace = True)\n",
    "\n",
    "# Correlation matrix (heatmap)\n",
    "corr_matrix = anemia_data.iloc[:,:-1].corr().round(2)\n",
    "plt.figure(figsize = (10, 5))\n",
    "sns.heatmap(corr_matrix, annot = True, center = 0, vmin = -1, vmax = 1)\n",
    "plt.show()\n",
    "\n",
    "anemia_data.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutp_outliers = anemia_data[anemia_data.NEUTp > 100].index\n",
    "hgb_outliers = anemia_data[anemia_data.HGB < 0].index\n",
    "hct_outliers = anemia_data[anemia_data.HCT > 100].index\n",
    "mcv_outliers = anemia_data[(anemia_data.MCV < 0) | (anemia_data.MCV > 200)].index\n",
    "mch_outliers = anemia_data[anemia_data.MCH > 100].index\n",
    "pct_outliers = anemia_data[anemia_data.PCT > 1].index\n",
    "pdw_outliers = anemia_data[anemia_data.PDW > 50].index\n",
    "\n",
    "# Removing outliers from the dataset\n",
    "combined_outliers = neutp_outliers.append([hgb_outliers, hct_outliers, mcv_outliers, mch_outliers, pct_outliers, pdw_outliers])\n",
    "anemia_data.drop(combined_outliers, inplace = True)\n",
    "\n",
    "for var in Xtrain.columns:\n",
    "    plot_box(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Dataset Outliers**\n",
    "\n",
    "- NEUTp has an otlier for \"Normocytic hypochromic anemia\" and \"Healthy\"\n",
    "\n",
    "- LYMn has an outlier for \"Iron deficiency anemia\"\n",
    "\n",
    "- NEUTn has an outlier for \"Iron deficiency anemia\" and \"Leukimea with thrombocytopenia\"\n",
    "\n",
    "- RBC has an outlier for \"Normocytic hypochromic anemia\" and \"Other microcytic anemia\"\n",
    "\n",
    "- HGB has a negative record for \"Iron deficiency anemia\" and two outliers for \"Other microcytic anemia\"\n",
    "\n",
    "- HCT has an outlier for \"Normocytic hypochromic anemia\"\n",
    "\n",
    "- MCV has a negative value for \"Iron deficiency anemia\"\n",
    "\n",
    "- MCH has outliers for \"Normocytic hypochromic anemia\" and \"Normocytic normochromic anemia\"\n",
    "\n",
    "- MCHC has two negative values for \"Other microcytic anemia\"\n",
    "\n",
    "- PDW has an outlier for \"Thrombocytopenia\"\n",
    "\n",
    "- PCT has outliers for \"Normocytic hypochromic anemia\" and \"Other microcytic anemia\"\n",
    "\n",
    "***\n",
    "\n",
    "Since the dataset is not normally distributed, and we are dealing with numeric data, a robust approach that doesn’t assume a normal distribution is ideal. Options:\n",
    "\n",
    "- Modified Z-Score (MAD) or IQR: These are simple yet effective, and they work well for detecting univariate outliers when you have skewed data.\n",
    "\n",
    "- Isolation Forest: For multi-dimensional data, Isolation Forest is a solid choice as it doesn’t require assumptions about distribution and can capture more complex relationships.\n",
    "\n",
    "- Local Outlier Factor (LOF): If you suspect that your dataset might have local patterns (e.g., different clusters of anemic types), then LOF could also be worth considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames to store the results for each outlier detection method\n",
    "iqr_df = pd.DataFrame(columns=['column', 'iqr', 'lower', 'upper', 'outlier_count'])\n",
    "mad_df = pd.DataFrame(columns=['column', 'mad', 'modified_z_threshold', 'outlier_count'])\n",
    "isolation_forest_df = pd.DataFrame(columns=['column', 'outlier_count'])\n",
    "lof_df = pd.DataFrame(columns=['column', 'outlier_count'])\n",
    "\n",
    "def out_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Interquartile Range (IQR) method.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The name of the column for which to detect outliers.\n",
    "    \n",
    "    Returns:\n",
    "    None: Updates the global iqr_df with IQR, lower and upper bounds, and outlier count.\n",
    "    \"\"\"\n",
    "    q25, q75 = np.quantile(df[column], 0.25), np.quantile(df[column], 0.75)\n",
    "    iqr = round(q75 - q25, 2)                                       # calculating the IQR\n",
    "    cut_off = iqr * 1.5                                             # calculating the outlier cutoff\n",
    "    lower, upper = round(q25 - cut_off, 2), round(q75 + cut_off, 2) # calculating the lower and upper bound value\n",
    "    \n",
    "    # Calculate the number of records below and above lower and upper bound value respectively\n",
    "    df1 = df[df[column] > upper]\n",
    "    df2 = df[df[column] < lower]\n",
    "    outlier_count = df1.shape[0] + df2.shape[0]\n",
    "    \n",
    "    # Append results to iqr_df\n",
    "    iqr_df.loc[len(iqr_df)] = [column, iqr, lower, upper, outlier_count]\n",
    "\n",
    "def modified_z_score(df, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Modified Z-Score method (based on Median Absolute Deviation).\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The name of the column for which to detect outliers.\n",
    "    \n",
    "    Returns:\n",
    "    None: Updates the global mad_df with MAD, threshold, and outlier count.\n",
    "    \"\"\"\n",
    "    median = df[column].median()\n",
    "    mad = np.median(np.abs(df[column] - median))\n",
    "    modified_z_scores = 0.6745 * (df[column] - median) / mad\n",
    "    threshold = 3.5\n",
    "    outliers = df[np.abs(modified_z_scores) > threshold]\n",
    "    outlier_count = outliers.shape[0]\n",
    "    \n",
    "    # Append results to mad_df\n",
    "    mad_df.loc[len(mad_df)] = [column, mad, threshold, outlier_count]\n",
    "\n",
    "def isolation_forest_outliers(df, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Isolation Forest algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The name of the column for which to detect outliers.\n",
    "    \n",
    "    Returns:\n",
    "    None: Updates the global isolation_forest_df with outlier count.\n",
    "    \"\"\"\n",
    "    model = IsolationForest(contamination = 0.05, random_state = 42)\n",
    "    df['anomaly'] = model.fit_predict(df[[column]])\n",
    "    outlier_count = df[df['anomaly'] == -1].shape[0]\n",
    "    \n",
    "    # Append results to isolation_forest_df\n",
    "    isolation_forest_df.loc[len(isolation_forest_df)] = [column, outlier_count]\n",
    "    df.drop(columns = ['anomaly'], inplace = True)\n",
    "\n",
    "def local_outlier_factor(df, column):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Local Outlier Factor (LOF) algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The name of the column for which to detect outliers.\n",
    "    \n",
    "    Returns:\n",
    "    None: Updates the global lof_df with outlier count.\n",
    "    \"\"\"\n",
    "    model = LocalOutlierFactor(n_neighbors = 20, contamination = 0.05)\n",
    "    df['anomaly'] = model.fit_predict(df[[column]])\n",
    "    outlier_count = df[df['anomaly'] == -1].shape[0]\n",
    "    \n",
    "    # Append results to lof_df\n",
    "    lof_df.loc[len(lof_df)] = [column, outlier_count]\n",
    "    df.drop(columns = ['anomaly'], inplace = True)\n",
    "\n",
    "# Apply all outlier detection methods to each numeric column in the anemia dataset\n",
    "for column in anemia_data.iloc[:, :-1].columns:\n",
    "    out_iqr(anemia_data, column)\n",
    "    modified_z_score(anemia_data, column)\n",
    "    isolation_forest_outliers(anemia_data, column)\n",
    "    local_outlier_factor(anemia_data, column)\n",
    "\n",
    "# Displaying all results\n",
    "display(iqr_df, mad_df, isolation_forest_df, lof_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Understanding the distribution under the Diagnosis column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a280437ad43e40baa4cb4adf7e78b715",
      "71dcfcc6ad814e9dbc6b7aadf2a82b69",
      "ba0ecb2ee3354aa3a2a8020062d011b7",
      "e0fabd3729f04d349d8c29ca1e647ef5",
      "6332529fbd7244008688b33eabf580c1",
      "3e8fbe7f0e494df8a4caed17286d3a63",
      "ccaeb4a5326d437ab152896b26935efb",
      "6ec75dd15c054db79fb356e57587aaec",
      "9440c1a2579349c8b545fe930b10bca3",
      "f41ee5fc20964e1c9331f970791e9284",
      "7258e3480b1e400fa299fb78f6f32792",
      "ebc2724c5e104e7c90c59f432d89eebe",
      "08c7173dfade46db9b3077e21787b6a4",
      "d5c6f22777eb4ab6a48bcbd0f03a37af",
      "233e6b1a98a249f1871a336dbb9d1bfa",
      "85330e014af84182a60d19c2c33db25d",
      "d3db0534bfb248bfbd40ae886a233413",
      "0dd62fc411c04fe4a39a2dc7a4e38bcf",
      "190a198f0d494f7d9c5be664d4c760ed",
      "173d61e7907444bd83ba1554f3995f1d",
      "b3c2933bd9d54661916034dfd8c2a6f9",
      "a59639f7fe844103a7fa254db8339385",
      "6a7d93fe0b564002b5ca07645aa18e6e",
      "e10fd7453ccf4f13a84bdadbc1e53a47",
      "96d4c1974af2449bbe392320ed136b88",
      "a56275fc6260460bb924a3c2fd40f80e",
      "0c78e59886144376956cfac3f22d02bf",
      "7ddf327b191a499da71d026a8f952afb",
      "d2b433c5cbed4357ace8c8a6ec68d408",
      "1efbc931c98d4e2884cc3f50b6341fdb",
      "e972ad247db742f28c3179dc5125402b",
      "d3cee06941ca48e5979c7aa520c4320d",
      "73ec7b30eb8041f38073ccde360ec48d",
      "8f6d4519fcce44169b61b1039bb65451",
      "db76b2394fea42078e252ad046f98e51",
      "ee35f6da46b942cc80fce3a218ed86f8",
      "abb6550c94434d31b5fe8d5f4416e8c6",
      "ed78d98ad63049568bd7c819989c0c1d",
      "50ca2b479313478cb1a8a40b4f79fe7b",
      "15aa682d7a864fe0badb8a2a20a2cc70",
      "3a8d0c91431d4e17aefa15dd746a77cf",
      "b40b80a2abc04910941fd953ff03b7c0",
      "0501eec8e5e7407f8ac56c9ffecbb0c0",
      "c2ed06b98f4241f6b7b2148ce3a46f25"
     ]
    },
    "id": "wwWcHOx1a_ly",
    "outputId": "2a54e2f5-5348-4593-cd60-a2a8758360af"
   },
   "outputs": [],
   "source": [
    "print(anemia_data.Diagnosis.value_counts(), \"\\n\")\n",
    "\n",
    "#anemia_type_dataset_profile = ProfileReport(anemia_data, title = \"Anemia Type Dataset\") # Generating a report of the data\n",
    "#anemia_type_dataset_profile.to_file(\"anemia_type_dataset_characteristics.html\")         # Saving the report to .html for inspection\n",
    "\n",
    "# visualizing the counts for each diagnosis in the dataset as a pie chart\n",
    "diagnosis_counts = anemia_data['Diagnosis'].value_counts().reset_index()\n",
    "diagnosis_counts.columns = ['Diagnosis', '# Cases']\n",
    "\n",
    "fig = px.pie(\n",
    "    diagnosis_counts, \n",
    "    values = '# Cases', \n",
    "    names = 'Diagnosis', \n",
    "    hole = 0.4,\n",
    "    title = 'Diagnosis Distribution',\n",
    "    template = 'plotly_dark'\n",
    ")\n",
    "fig.update_layout(title_font = dict(size = 20, color = 'white', family = \"Arial\"),\n",
    "                  legend_title_text = 'Diagnosis',\n",
    "                  legend = dict(font = dict(color = 'white')),\n",
    "                  width = 800, height = 500)\n",
    "fig.show()\n",
    "\n",
    "anemia_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Imbalance**\n",
    "\n",
    "The dataset is heavily imbalanced under the 'Diagnosis' column.\n",
    "\n",
    "This may hinder the subsequent training of ML models and lead to misleading results (i.e favourisation of the majority class).\n",
    "\n",
    "Therefore, we will need to apply SMOTE on the training data only (to avoid data leakage and ensure real-world testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Correlation Ranges and Their Interpretation:**\n",
    "\n",
    "***Perfect Positive Correlation (+1):***\n",
    "\n",
    "* This means that two features move together perfectly; if one feature increases, the other feature always increases in a directly proportional way.\n",
    "Example: If A and B have a correlation of +1, then as A increases, B always increases in the exact same manner.\n",
    "\n",
    "***High Positive Correlation (+0.7 to +1):***\n",
    "\n",
    "* Strong relationship where an increase in one feature is highly likely to be accompanied by an increase in the other feature.\n",
    "* Action: Investigate if features are redundant and consider dropping one of the features if they contain similar information.\n",
    "\n",
    "***Moderate Positive Correlation (+0.4 to +0.7):***\n",
    "\n",
    "* There is a clear positive relationship, but it is not perfect. These features may still contain useful independent information.\n",
    "* Action: Generally, no need to drop either feature unless domain knowledge suggests redundancy.\n",
    "\n",
    "***Low Positive Correlation (+0.1 to +0.4):***\n",
    "\n",
    "* Weak positive relationship; the features increase together, but only slightly.\n",
    "* Action: Low concern for multicollinearity. Keep both features unless otherwise indicated.\n",
    "\n",
    "***No Correlation (-0.1 to +0.1):***\n",
    "\n",
    "* No discernible linear relationship between the features.\n",
    "* Action: Both features can coexist without causing issues of multicollinearity.\n",
    "\n",
    "***Low Negative Correlation (-0.1 to -0.4):***\n",
    "\n",
    "* Weak inverse relationship; as one feature increases, the other tends to decrease slightly.\n",
    "* Action: Similar to weak positive correlation, usually not a concern.\n",
    "\n",
    "***Moderate Negative Correlation (-0.4 to -0.7):***\n",
    "\n",
    "* Clear inverse relationship; as one feature increases, the other decreases in a moderate, predictable way.\n",
    "* Action: Consider if the features are providing redundant information in an inverse way.\n",
    "\n",
    "***High Negative Correlation (-0.7 to -1):***\n",
    "\n",
    "* Strong inverse relationship; as one feature increases, the other decreases in a very predictable and proportional way.\n",
    "* Action: Similar to high positive correlation, you may need to drop or combine features to reduce redundancy.\n",
    "\n",
    "***Perfect Negative Correlation (-1):***\n",
    "\n",
    "* This means that two features are perfectly inversely correlated. As one increases, the other decreases in exact proportion.\n",
    "* Example: If A and B have a correlation of -1, then as A increases, B always decreases by the same amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise correlation with the target variable\n",
    "anemia_data_copy = anemia_data.copy()\n",
    "\n",
    "# encoding the target column\n",
    "label_encoder = LabelEncoder()\n",
    "anemia_data_copy['Diagnosis'] = label_encoder.fit_transform(anemia_data_copy['Diagnosis'])\n",
    "\n",
    "# List of all features (excluding 'Diagnosis' and the control variable 'HGB')\n",
    "features = [col for col in anemia_data_copy.columns if col not in ['Diagnosis', 'HGB']]\n",
    "\n",
    "# Perform partial correlation between each feature and 'Diagnosis', controlling for 'HGB'\n",
    "partial_corr_results = {}\n",
    "for feature in features:\n",
    "    result = pg.partial_corr(data=anemia_data_copy, x=feature, y='Diagnosis', covar='HGB')\n",
    "    partial_corr_results[feature] = result['r'].values[0]\n",
    "\n",
    "partial_corr_df = pd.DataFrame(partial_corr_results.items(), columns=['Feature', 'Partial Correlation (r)'])\n",
    "partial_corr_df = round(partial_corr_df.sort_values(by='Partial Correlation (r)', ascending=False), 2)\n",
    "partial_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**r (Partial Correlation Coefficient):**\n",
    "\n",
    "This is the main result: the partial correlation coefficient between 'RBC' and 'Diagnosis', while controlling for 'HGB'.\n",
    "\n",
    "Range: The value of r ranges from -1 to 1.\n",
    "\n",
    "* r = 1: Perfect positive correlation (as 'RBC' increases, 'Diagnosis' increases, after controlling for 'HGB').\n",
    "  \n",
    "* r = -1: Perfect negative correlation (as 'RBC' increases, 'Diagnosis' decreases, after controlling for 'HGB').\n",
    "  \n",
    "* r = 0: No linear relationship between 'RBC' and 'Diagnosis', after controlling for 'HGB'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the features (X) and the target variable (y)\n",
    "X = anemia_data.drop(columns = ['Diagnosis'])\n",
    "y = anemia_data['Diagnosis']\n",
    "\n",
    "# Adding a constant to the features to account for the intercept\n",
    "X_with_constant = pd.concat([pd.DataFrame({'Intercept': 1}, index = X.index), X], axis=1)\n",
    "\n",
    "# Calculating VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = anemia_data.iloc[:, :-1].columns # all columns but the last which is the diagnosis (categorical)\n",
    "vif_data['VIF'] = [np.round(variance_inflation_factor(X_with_constant.values, i), 2) for i in range(1, X_with_constant.shape[1])]\n",
    "\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Variance Inflation Factor (VIF)** measures the severity of multicollinearity in regression analysis. It is a statistical concept that indicates the increase in the variance of a regression coefficient as a result of collinearity.\n",
    "\n",
    "**Interpreting VIF Values:**\n",
    "\n",
    "* VIF = 1: No multicollinearity.\n",
    "\n",
    "* VIF between 1 and 5: Moderate multicollinearity (usually acceptable).\n",
    "\n",
    "* VIF > 5: High multicollinearity (you should consider removing or combining highly correlated variables).\n",
    "\n",
    "* VIF > 10: Indicates severe multicollinearity, which is typically problematic.\n",
    "\n",
    "***\n",
    "\n",
    "### Considerations\n",
    "\n",
    "**1. Addressing dataset imbalance through SMOTE**\n",
    "\n",
    "Let's think about this in terms of baking a cake. Imagine you're trying to bake a cake (your predictive model) using a recipe (the learning algorithm) but your ingredients (the training data) aren't quite right. Let's say you have way too many eggs (the majority class) and not enough flour (the minority class).\n",
    "\n",
    "SMOTE is like a magical kitchen gadget that can generate more flour for you. It looks at the flour you already have (the minority class), and creates similar, but not identical, new flour (synthetic minority samples). Now, you have a better balance of eggs and flour. Awesome, right?\n",
    "\n",
    "But here's the catch: you also want to save a bit of the original eggs and flour (your test set) to make a tiny cupcake (to test your model), and you don't want to use the magic gadget on these. Why? Because the cupcake is like the unseen, real-world data you'll be dealing with. Using the gadget on it would be like pretending you have more diverse real-world data than you really do, and you might end up with a model that performs well on your synthetic data but is a total flop on real-world data.\n",
    "\n",
    "So, in your ML baking adventure, use SMOTE after you've split your data into a training set and a test set. Apply SMOTE only to the training set, keeping your test set untouched and reflective of the real-world data distribution. This way, your 'cake' (model) will be more likely to perform well not only on your balanced training data but also on your untouched test data.\n",
    "\n",
    "Remember, ML is a lot like baking. You're constantly adjusting, balancing ingredients, and sometimes having to start from scratch.\n",
    "\n",
    "***\n",
    "\n",
    "**2. Scaler**\n",
    "\n",
    "* Use RobustScaler() if your data has outliers and you want a robust way to scale your features without being skewed by extreme values. RobustScaler() will scale the data using the median and IQR, making it less sensitive to outliers.\n",
    "  \n",
    "* Use MinMaxScaler() if your models benefit from having input features strictly within a defined range (especially neural networks), but be cautious of outliers. MinMaxScaler() will scale the data between 0 and 1.\n",
    "\n",
    "* Use StandardScaler() if you are working with models that assume normally distributed data and your features are relatively normally distributed without extreme outliers. StandardScaler() will center the data around 0 with a standard deviation of 1.\n",
    "\n",
    "***\n",
    "\n",
    "**3. Evaluation Metrics**\n",
    "\n",
    "***micro:***\n",
    "- Aggregates all TP, FP, FN across classes and calculates the metric globally. Treats every sample equally, making it suitable when you care about the overall performance on individual samples.\n",
    "\n",
    "***macro:***\n",
    "- Averages the metric for each class equally, regardless of how many samples are in each class. Useful when you care about all classes equally.\n",
    "\n",
    "***weighted:***\n",
    "- Like macro, but accounts for the number of samples in each class. It's good when you want to consider class imbalance while still evaluating each class individually.\n",
    "\n",
    "***\n",
    "\n",
    "**4. Model Considerations**\n",
    "\n",
    "Generally, decision tree-based algorithms perform well on imbalanced datasets. Similarly bagging and boosting based techniques are good choices for imbalanced classification problems.\n",
    "\n",
    "- Random Forest: This ensemble method can handle imbalances through its inherent structure, and it can also be tuned to weigh classes differently.\n",
    "\n",
    "- Gradient Boosting Machines (GBM): Similar to Random Forest, GBMs can be adjusted with parameters like scale_pos_weight in XGBoost to give more weight to the minority class.\n",
    "\n",
    "- SVMs can be effective for imbalanced datasets by using a cost-sensitive approach, where a higher penalty is assigned to misclassifying the minority class.\n",
    "\n",
    "- Logistic regression can be adapted for imbalanced datasets by using class weights to penalize mistakes on the minority class more heavily.\n",
    "\n",
    "- KNN can be effective, especially if you use distance weighting to give more importance to the minority class during classification.\n",
    "\n",
    "***\n",
    "\n",
    "**5. Techniques to improve performance**\n",
    "\n",
    "- Anomaly Detection Techniques: When the minority class is very small, treating it as an anomaly can lead to better performance. Anomaly detection problems consider the minority class(rare-events) as outliers and apply several approaches to detect them.\n",
    "\n",
    "- ***Loss function adjustments*** : put an additional cost every time the model misclassifies the minority class. \r\n",
    "Every model works in a different way and there is a different way to apply this trick to every model. Setting up this penalty cost value could be complex, you might need to try out multiple schemes and check what works best for your case.\n",
    "\n",
    "- Tuning the hyperparameters of the gradient boosting models can be achieved using the \"Hyperopt\" libary.\n",
    "\n",
    "- ***imblearn*** package (https://imbalanced-learn.org/stable/)\n",
    "\n",
    "- ***Morse-Smale Regression***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ML Training with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = anemia_data.drop(columns = ['Diagnosis'])\n",
    "y = anemia_data['Diagnosis']\n",
    "\n",
    "original_columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) # artificially increasing the sample size\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled) # fit on training data\n",
    "X_test = scaler.transform(X_test)                           # transform on test data\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_resampled = label_encoder.fit_transform(y_train_resampled)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "\n",
    "smote_counts_original = pd.Series(label_encoder.inverse_transform(y_train_resampled)).value_counts()\n",
    "\n",
    "print(\"\\nCounts of each class after SMOTE (original labels):\")\n",
    "print(smote_counts_original)\n",
    "\n",
    "model_pipeline = [\n",
    "    XGBClassifier(),\n",
    "    CatBoostClassifier(verbose = 200),\n",
    "    #SVC(kernel = 'rbf', C = 1, gamma = 'scale', probability = True),\n",
    "    #KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "model_list = ['XGBoost', 'CatBoost', 'Decision Tree', 'Random Forest'] # defining all models that are to be fitted\n",
    "acc_list = []        # to store the Accuracy for each model\n",
    "precision_list = []  # to store the Precission for each model\n",
    "recall_list = []     # to store the Recall for each model\n",
    "f1_list = []         # to store the F1 Score for each model\n",
    "\n",
    "times = []           # to store the computation times for each model\n",
    "auc_list = []        # to store the are under the curve for each model\n",
    "cm_list = []         # to store the confusion matrix for each model\n",
    "\n",
    "cf_reports = []      # to store the classification reports for each model\n",
    "feature_impor = []   # to store the feature importances for each model \n",
    "\n",
    "y_test_binarized = label_binarize(y_test, classes = range(len(label_encoder.classes_)))\n",
    "\n",
    "feature_importances_dicts = []\n",
    "\n",
    "for model in model_pipeline:                         # iterating through all ML models\n",
    "    start_time = time.time()                         # recording the start time of the model training\n",
    "    model.fit(X_train_resampled, y_train_resampled)  # training (fitting) the model on the resampled training set\n",
    "    times.append(round(time.time() - start_time, 4)) # storing the training time of each model\n",
    "    y_pred = model.predict(X_test)                   # making predictions on the original test set\n",
    "\n",
    "    # appending lists with their respective evaluation metrics rounded to 3 decimal places\n",
    "    acc_list.append(round(accuracy_score(y_test, y_pred), 3))\n",
    "    precision_list.append(round(precision_score(y_test, y_pred, average = 'macro'), 3))\n",
    "    recall_list.append(round(recall_score(y_test, y_pred, average = 'macro'), 3))\n",
    "    f1_list.append(round(f1_score(y_test, y_pred, average = 'macro'), 3))\n",
    "\n",
    "    # AUC Calculation\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "    except AttributeError:\n",
    "        if hasattr(model, 'decision_function'):\n",
    "            y_proba = model.decision_function(X_test)\n",
    "        else:\n",
    "            y_proba = None # skipping AUC calculation for non-probabilistic models\n",
    "\n",
    "    if y_proba is not None:\n",
    "        auc_list.append(round(roc_auc_score(y_test_binarized, y_proba, average = 'macro', multi_class = 'ovr'), 3))\n",
    "    else:\n",
    "        auc_list.append(None)  # assigning a placeholder value if y_proba is None\n",
    "\n",
    "    cm_list.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    report_dict = classification_report(y_test, y_pred, target_names = label_encoder.classes_, output_dict = True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df[['precision', 'recall', 'f1-score']] = report_df[['precision', 'recall', 'f1-score']].round(2)\n",
    "    report_df['Model'] = model_list[model_pipeline.index(model)]\n",
    "    report_df['Class'] = report_df.index\n",
    "    cf_reports.append(report_df)\n",
    "\n",
    "    # Checking if the model has feature importances (SVM, KNN, and Naive Bayes do not have feature importances)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        importance_dict = dict(zip(original_columns, importances)) # Creating a dictionary where keys are feature names and values are the importances\n",
    "        importance_dict['Model'] = model_list[model_pipeline.index(model)]\n",
    "        feature_importances_dicts.append(importance_dict)  # appending the dictionary to the list\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = model.coef_[0]\n",
    "    else:\n",
    "        importances = None\n",
    "\n",
    "if feature_importances_dicts:\n",
    "    feature_importances_df = pd.DataFrame(feature_importances_dicts)\n",
    "\n",
    "cf_reports_df = pd.concat(cf_reports, ignore_index = True)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'Model' : model_list,\n",
    "    'Accuracy' : acc_list,\n",
    "    'Precision' : precision_list,\n",
    "    'Recall' : recall_list,\n",
    "    'F1 Score' : f1_list,\n",
    "    'AUC Score' : auc_list,\n",
    "    'Training Time (s)' : times\n",
    "})\n",
    "\n",
    "result_df.sort_values(by = 'Accuracy', ascending = False, inplace = True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(y_train), '\\n')\n",
    "print(Counter(y_test))\n",
    "print(\"\\n\", 100 * \"-\", \"\\n\")\n",
    "\n",
    "# Normalizing feature importances to sum to 100%\n",
    "feature_importances_df.iloc[:, :-1] = (feature_importances_df.iloc[:, :-1].div(feature_importances_df.iloc[:, :-1].sum(axis=1), axis=0) * 100).round(1)\n",
    "\n",
    "feature_importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (18, 15))\n",
    "\n",
    "decoded_labels = label_encoder.inverse_transform([i for i in range(len(label_encoder.classes_))])\n",
    "\n",
    "for i in range(len(cm_list)):\n",
    "    cm = cm_list[i]\n",
    "    model = model_list[i]\n",
    "    \n",
    "    sub = fig.add_subplot(2, 2, i + 1).set_title(model)\n",
    "    \n",
    "    cm_plot = sns.heatmap(cm, annot = True, cmap = 'Blues_r', xticklabels = decoded_labels, yticklabels = decoded_labels)\n",
    "    cm_plot.set_xlabel('Predicted Values')\n",
    "    cm_plot.set_ylabel('Actual Values')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Filter out non-diagnosis rows from the cf_reports_df\n",
    "diagnosis_classes = label_encoder.classes_  # These are the actual diagnosis labels\n",
    "\n",
    "# Filter the DataFrame to keep only rows where 'Class' is in the actual diagnosis labels\n",
    "cf_reports_df = cf_reports_df[cf_reports_df['Class'].isin(diagnosis_classes)]\n",
    "\n",
    "# Now cf_reports_df will only contain rows corresponding to your diagnosis labels\n",
    "pd.set_option('display.max_rows', None)\n",
    "cf_reports_df.head(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Explanation for the class imbalance observed across the test set:**\n",
    "\n",
    "Even though we applied SMOTE to the training set, our test set remains imbalanced, reflecting the original distribution of the data. \n",
    "Since SMOTE was applied only on the training data, the model learned from a balanced dataset, but when it’s evaluated on the imbalanced test set, predictions may favor the more common class labels in the test data.\n",
    "\n",
    "Why this happens:\n",
    "\n",
    "SMOTE only affects the training data to help the model learn from a more balanced distribution. The model is then tested on the original imbalanced data, which means it still encounters more examples of the common classes in the test set, making it harder to accurately predict the rarer classes.\n",
    "***\n",
    "**2. Model Bias towards Majority Classes:**\n",
    "Some models may inherently struggle to learn patterns for minority classes even after oversampling. For example, decision trees and ensemble methods (like Random Forests) can still be biased towards the majority classes due to the nature of how splits are made and how samples are distributed during training.\n",
    "\n",
    "Why this happens:\n",
    "\n",
    "Certain algorithms might not generalize well to minority classes even after balancing during training. This could be due to how they form their decision boundaries or the way they treat majority vs. minority class data. This might result in better classification for the majority classes but poor performance for the minority classes, leading to an imbalance in the confusion matrix.\n",
    "***\n",
    "**3. Performance and Overfitting:**\n",
    "\n",
    "Sometimes, oversampling can lead to overfitting to the minority class in the training data, but the model still struggles to generalize to the test set. Even though SMOTE helps by synthetically generating more data points, the minority class might still be harder to predict in real test data if the characteristics are more nuanced or complex.\n",
    "\n",
    "Why this happens:\n",
    "\n",
    "SMOTE does not create entirely new data but generates synthetic samples between existing minority samples. If these synthetically generated samples do not fully capture the complexity of the minority class, the model may still have difficulties predicting those instances in the test set.3. Performance and Overfitting:**\n",
    "Sometimes, oversampling can lead to overfitting to the minority class in the training data, but the model still struggles to generalize to the test set. Even though SMOTE helps by synthetically generating more data points, the minority class might still be harder to predict in real test data if the characteristics are more nuanced or complex.\n",
    "\n",
    "Why this happens:\n",
    "\n",
    "SMOTE does not create entirely new data but generates synthetic samples between existing minority samples. If these synthetically generated samples do not fully capture the complexity of the minority class, the model may still have difficulties predicting those instances in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Models with balanced class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models with balanced class weights\n",
    "weighted_models_pipeline = [\n",
    "    RandomForestClassifier(class_weight = 'balanced'),\n",
    "    SVC(class_weight = 'balanced'),\n",
    "    DecisionTreeClassifier(class_weight = 'balanced')\n",
    "]\n",
    "\n",
    "# Corresponding model names\n",
    "weighted_model_list = ['Random Forest (Balanced)', 'SVM (Balanced)', 'Decision Tree (Balanced)']\n",
    "\n",
    "# List to store results for the weighted models\n",
    "acc_list_weighted = []\n",
    "precision_list_weighted = []\n",
    "recall_list_weighted = []\n",
    "f1_list_weighted = []\n",
    "auc_list_weighted = []\n",
    "times_weighted = []\n",
    "cm_list_weighted = []\n",
    "cf_reports_weighted = []\n",
    "\n",
    "# Loop through the models with balanced class weights\n",
    "for i, model in enumerate(weighted_models_pipeline):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit the model on the resampled training set\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    times_weighted.append(round(time.time() - start_time, 4))\n",
    "    \n",
    "    # Predict on the original test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc_list_weighted.append(round(accuracy_score(y_test, y_pred), 3))\n",
    "    precision_list_weighted.append(round(precision_score(y_test, y_pred, average = 'macro'), 3))\n",
    "    recall_list_weighted.append(round(recall_score(y_test, y_pred, average = 'macro'), 3))\n",
    "    f1_list_weighted.append(round(f1_score(y_test, y_pred, average = 'macro'), 3))\n",
    "    \n",
    "    # Try to calculate AUC if applicable\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "    except AttributeError:\n",
    "        y_proba = model.decision_function(X_test)\n",
    "    \n",
    "    auc_list_weighted.append(round(roc_auc_score(y_test_binarized, y_proba, average = 'macro', multi_class = 'ovr'), 3))\n",
    "    cm_list_weighted.append(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    # Generate classification report\n",
    "    report_dict = classification_report(y_test, y_pred, target_names = label_encoder.classes_, output_dict = True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df[['precision', 'recall', 'f1-score']] = report_df[['precision', 'recall', 'f1-score']].round(2)\n",
    "    report_df['Model'] = weighted_model_list[i]\n",
    "    report_df['Class'] = report_df.index\n",
    "    cf_reports_weighted.append(report_df)\n",
    "\n",
    "# Combine the results into a DataFrame for weighted models\n",
    "weighted_result_df = pd.DataFrame({\n",
    "    'Model': weighted_model_list,\n",
    "    'Accuracy': acc_list_weighted,\n",
    "    'Precision': precision_list_weighted,\n",
    "    'Recall': recall_list_weighted,\n",
    "    'F1 Score': f1_list_weighted,\n",
    "    'AUC Score': auc_list_weighted,\n",
    "    'Training Time (s)': times_weighted\n",
    "})\n",
    "\n",
    "# Append the weighted models' results to the existing result_df\n",
    "result_df = pd.concat([result_df, weighted_result_df], ignore_index = True)\n",
    "\n",
    "# Combine the classification reports\n",
    "cf_reports_df = pd.concat(cf_reports + cf_reports_weighted, ignore_index = True)\n",
    "\n",
    "result_df.sort_values(by = 'Accuracy', ascending = False, inplace = True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Training on Non-Correlated Features & Running the Models on Patient Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data = pd.read_excel('patient_data_october.xlsx')\n",
    "patient_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping LYMp and NEUTp due to high correlation\n",
    "#anemia_data.drop(columns = ['LYMp', 'NEUTp'], inplace = True)\n",
    "\n",
    "X = anemia_data.drop(columns = ['Diagnosis'])\n",
    "y = anemia_data['Diagnosis']\n",
    "\n",
    "original_columns = X.columns\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "smote = SMOTE(random_state = 42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) # artificially increasing the sample size\n",
    "# Converting the resampled arrays into DataFrame for easier saving\n",
    "X_train_resampled_df = pd.DataFrame(X_train_resampled, columns = X.columns)\n",
    "y_train_resampled_df = pd.DataFrame(y_train_resampled, columns = ['Diagnosis'])\n",
    "\n",
    "# Saving the resampled datasets\n",
    "X_train_resampled_df.to_csv('X_train_resampled.csv', index = False)\n",
    "y_train_resampled_df.to_csv('y_train_resampled.csv', index = False)\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled) # fit on training data\n",
    "X_test = scaler.transform(X_test)                           # transform on test data\n",
    "joblib.dump(scaler, 'scaler.joblib')                        # saving the scaler\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_resampled = label_encoder.fit_transform(y_train_resampled)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')          # saving the label encoder\n",
    "\n",
    "smote_counts_original = pd.Series(label_encoder.inverse_transform(y_train_resampled)).value_counts()\n",
    "\n",
    "print(\"\\nCounts of each class after SMOTE (original labels):\")\n",
    "print(smote_counts_original)\n",
    "\n",
    "model_pipeline = [\n",
    "    XGBClassifier(),\n",
    "    CatBoostClassifier(verbose = 200),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier()\n",
    "]\n",
    "\n",
    "model_list = ['XGBoost', 'CatBoost', 'Decision Tree', 'Random Forest'] # defining all models that are to be fitted\n",
    "acc_list = []        # to store the Accuracy for each model\n",
    "precision_list = []  # to store the Precission for each model\n",
    "recall_list = []     # to store the Recall for each model\n",
    "f1_list = []         # to store the F1 Score for each model\n",
    "\n",
    "times = []           # to store the computation times for each model\n",
    "auc_list = []        # to store the are under the curve for each model\n",
    "cm_list = []         # to store the confusion matrix for each model\n",
    "\n",
    "cf_reports = []      # to store the classification reports for each model\n",
    "feature_impor = []   # to store the feature importances for each model \n",
    "\n",
    "y_test_binarized = label_binarize(y_test, classes = range(len(label_encoder.classes_)))\n",
    "\n",
    "feature_importances_dicts = []\n",
    "\n",
    "model_save_path = './saved_models/'                        # directory to save models\n",
    "os.makedirs(model_save_path, exist_ok = True)              # ensuring the directory exists\n",
    "\n",
    "for model_name, model in zip(model_list, model_pipeline):  # iterating through all ML models\n",
    "    start_time = time.time()                               # recording the start time of the model training\n",
    "    model.fit(X_train_resampled, y_train_resampled)        # training (fitting) the model on the resampled training set\n",
    "    times.append(round(time.time() - start_time, 4))       # storing the training time of each model\n",
    "    y_pred = model.predict(X_test)                         # making predictions on the original test set\n",
    "\n",
    "    # appending lists with their respective evaluation metrics rounded to 3 decimal places\n",
    "    acc_list.append(round(accuracy_score(y_test, y_pred), 3))\n",
    "    precision_list.append(round(precision_score(y_test, y_pred, average = 'macro'), 3))\n",
    "    recall_list.append(round(recall_score(y_test, y_pred, average = 'macro'), 3))\n",
    "    f1_list.append(round(f1_score(y_test, y_pred, average = 'macro'), 3))\n",
    "\n",
    "    # AUC Calculation\n",
    "    try:\n",
    "        y_proba = model.predict_proba(X_test)\n",
    "    except AttributeError:\n",
    "        if hasattr(model, 'decision_function'):\n",
    "            y_proba = model.decision_function(X_test)\n",
    "        else:\n",
    "            y_proba = None # skipping AUC calculation for non-probabilistic models\n",
    "\n",
    "    if y_proba is not None:\n",
    "        auc_list.append(round(roc_auc_score(y_test_binarized, y_proba, average = 'macro', multi_class = 'ovr'), 3))\n",
    "    else:\n",
    "        auc_list.append(None)  # assigning a placeholder value if y_proba is None\n",
    "\n",
    "    cm_list.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    report_dict = classification_report(y_test, y_pred, target_names = label_encoder.classes_, output_dict = True)\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df[['precision', 'recall', 'f1-score']] = report_df[['precision', 'recall', 'f1-score']].round(2)\n",
    "    report_df['Model'] = model_name\n",
    "    report_df['Class'] = report_df.index\n",
    "    cf_reports.append(report_df)\n",
    "\n",
    "    # Checking if the model has feature importances (SVM, KNN, and Naive Bayes do not have feature importances)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        importance_dict = dict(zip(original_columns, importances)) # Creating a dictionary where keys are feature names and values are the importances\n",
    "        importance_dict['Model'] = model_name\n",
    "        feature_importances_dicts.append(importance_dict)  # appending the dictionary to the list\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = model.coef_[0]\n",
    "    else:\n",
    "        importances = None\n",
    "\n",
    "    model_filename = model_save_path + model_name + '.joblib' # saving each trained model\n",
    "    joblib.dump(model, model_filename)\n",
    "\n",
    "if feature_importances_dicts:\n",
    "    feature_importances_df = pd.DataFrame(feature_importances_dicts)\n",
    "\n",
    "cf_reports_df = pd.concat(cf_reports, ignore_index = True)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'Model' : model_list,\n",
    "    'Accuracy' : acc_list,\n",
    "    'Precision' : precision_list,\n",
    "    'Recall' : recall_list,\n",
    "    'F1 Score' : f1_list,\n",
    "    'AUC Score' : auc_list,\n",
    "    'Training Time (s)' : times\n",
    "})\n",
    "\n",
    "result_df.sort_values(by = 'Accuracy', ascending = False, inplace = True)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the patient data with the same scaler used for training the models to ensure the features are on the same scale\n",
    "patient_data_scaled = scaler.transform(patient_data)\n",
    "\n",
    "# List to store the results (Model name and corresponding prediction)\n",
    "predicted_diagnosis = []\n",
    "\n",
    "for i, model in enumerate(model_pipeline):\n",
    "    patient_pred = model.predict(patient_data_scaled).ravel()            # predicting the diagnosis for the patient\n",
    "    patient_pred = label_encoder.inverse_transform(patient_pred) # decoding the prediction from the encoded labels back to the original diagnosis\n",
    "    predicted_diagnosis.append({\n",
    "        'Model': model_list[i],\n",
    "        'Prediction': patient_pred[0]\n",
    "    })\n",
    " \n",
    "patient_result_df = pd.DataFrame(predicted_diagnosis) # converting the list of dictionaries to a DataFrame\n",
    "patient_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the Gradio inputs and the predict_diagnosis function arguments accordingly\n",
    "input_fields = [gr.Number(label = col) for col in original_columns]\n",
    "\n",
    "# function for diagnosis prediction based on blood test results\n",
    "def predict_diagnosis(*args):\n",
    "    try:\n",
    "        patient_data = pd.DataFrame([args], columns = original_columns)\n",
    "        patient_data_scaled = scaler.transform(patient_data)\n",
    "        predicted_diagnosis = []\n",
    "\n",
    "        for i, model in enumerate(model_pipeline):\n",
    "            patient_pred = model.predict(patient_data_scaled).ravel()\n",
    "            patient_pred = label_encoder.inverse_transform(patient_pred)\n",
    "            predicted_diagnosis.append(f\"{model_list[i]}: {patient_pred[0]}\")\n",
    "\n",
    "        return \"\\n\".join(predicted_diagnosis)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", str(e))\n",
    "        return f\"Error: {str(e)}\", \"An error occurred while generating the classification reports.\"\n",
    "\n",
    "# Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn = predict_diagnosis,                      # the function to wrap a user interface (UI) around (predict_diagnosis)\n",
    "    inputs = input_fields,                       # the Gradio component(s) to use for the input\n",
    "    outputs = gr.Textbox(label = \"Predictions\"), # the Gradio component(s) to use for the output.\n",
    "    title = \"Anemia Diagnosis Prediction\",\n",
    "    description = \"Enter your blood test results to get a diagnosis prediction from various models.\"\n",
    ")\n",
    "\n",
    "iface.launch(share = True) # creating a publicly accessible URL for the demo (expires after 72 hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import nb_export\n",
    "nb_export('app.ipynb', lib_path = '.', name = 'app')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
